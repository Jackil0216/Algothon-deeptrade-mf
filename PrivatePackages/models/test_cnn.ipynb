{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2019],\n",
      "          [-0.2019],\n",
      "          [-0.2019],\n",
      "          [-0.2019]],\n",
      "\n",
      "         [[-0.1466],\n",
      "          [-0.1466],\n",
      "          [-0.1466],\n",
      "          [-0.1466]],\n",
      "\n",
      "         [[ 0.2636],\n",
      "          [ 0.2636],\n",
      "          [ 0.2636],\n",
      "          [ 0.2636]]],\n",
      "\n",
      "\n",
      "        [[[-0.2019],\n",
      "          [-0.2019],\n",
      "          [-0.2019],\n",
      "          [-0.2019]],\n",
      "\n",
      "         [[-0.1466],\n",
      "          [-0.1466],\n",
      "          [-0.1466],\n",
      "          [-0.1466]],\n",
      "\n",
      "         [[ 0.2636],\n",
      "          [ 0.2636],\n",
      "          [ 0.2636],\n",
      "          [ 0.2636]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a sample batch with two images\n",
    "sample = np.array([[\n",
    "                [[0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3]]\n",
    "                ],\n",
    "                [\n",
    "                [[0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3]]\n",
    "                ]])\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "\n",
    "# Define the convolution layer\n",
    "conv2d = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=[2, 3], stride=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply the convolution operation\n",
    "output = conv2d(sample_tensor)\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2019],\n",
      "          [-0.2019]],\n",
      "\n",
      "         [[-0.1466],\n",
      "          [-0.1466]],\n",
      "\n",
      "         [[ 0.2636],\n",
      "          [ 0.2636]]],\n",
      "\n",
      "\n",
      "        [[[-0.2019],\n",
      "          [-0.2019]],\n",
      "\n",
      "         [[-0.1466],\n",
      "          [-0.1466]],\n",
      "\n",
      "         [[ 0.2636],\n",
      "          [ 0.2636]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "maxpool = torch.nn.MaxPool2d(kernel_size=[2, 1], stride=2)\n",
    "\n",
    "output2 = maxpool(output)\n",
    "\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.4077]],\n",
      "\n",
      "         [[ 0.3020]],\n",
      "\n",
      "         [[ 0.2059]],\n",
      "\n",
      "         [[ 0.0535]],\n",
      "\n",
      "         [[-0.3177]],\n",
      "\n",
      "         [[ 0.4963]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4077]],\n",
      "\n",
      "         [[ 0.3020]],\n",
      "\n",
      "         [[ 0.2059]],\n",
      "\n",
      "         [[ 0.0535]],\n",
      "\n",
      "         [[-0.3177]],\n",
      "\n",
      "         [[ 0.4963]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv2d_2 = torch.nn.Conv2d(in_channels=3, out_channels=3*2, kernel_size=[2, 1], stride=1)\n",
    "\n",
    "output3 = conv2d_2(output2)\n",
    "\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4077,  0.3020,  0.2059,  0.0535, -0.3177,  0.4963],\n",
       "        [ 0.4077,  0.3020,  0.2059,  0.0535, -0.3177,  0.4963]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3.squeeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4312, 0.1090, 0.0821, 0.2245, 0.1628, 0.0890],\n",
       "        [0.4312, 0.1090, 0.0821, 0.2245, 0.1628, 0.0890]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(output3.squeeze(-1).squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953]],\n",
      "\n",
      "         [[ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285]],\n",
      "\n",
      "         [[ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603]]],\n",
      "\n",
      "\n",
      "        [[[-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953]],\n",
      "\n",
      "         [[ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285]],\n",
      "\n",
      "         [[ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a sample batch with two images\n",
    "sample = np.array([[\n",
    "                [[0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3]]\n",
    "                ],\n",
    "                [\n",
    "                [[0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3]]\n",
    "                ]])\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "\n",
    "# Define the convolution layer\n",
    "conv2d = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=[2, 1], stride=1)\n",
    "\n",
    "# Apply the convolution operation\n",
    "output = conv2d(sample_tensor)\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953]],\n",
      "\n",
      "         [[ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285]],\n",
      "\n",
      "         [[ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603]]],\n",
      "\n",
      "\n",
      "        [[[-0.2701, -0.2827, -0.2953],\n",
      "          [-0.2701, -0.2827, -0.2953]],\n",
      "\n",
      "         [[ 0.0574,  0.0144, -0.0285],\n",
      "          [ 0.0574,  0.0144, -0.0285]],\n",
      "\n",
      "         [[ 0.2419,  0.2511,  0.2603],\n",
      "          [ 0.2419,  0.2511,  0.2603]]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "maxpool = torch.nn.MaxPool2d(kernel_size=[2, 1], stride=[2, 1])\n",
    "\n",
    "output2 = maxpool(output)\n",
    "\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.3801,  0.3779,  0.3757]],\n",
      "\n",
      "         [[-0.2949, -0.2924, -0.2900]],\n",
      "\n",
      "         [[-0.1774, -0.1755, -0.1736]],\n",
      "\n",
      "         [[ 0.2811,  0.2837,  0.2863]],\n",
      "\n",
      "         [[ 0.0463,  0.0316,  0.0168]],\n",
      "\n",
      "         [[-0.2090, -0.2092, -0.2094]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3801,  0.3779,  0.3757]],\n",
      "\n",
      "         [[-0.2949, -0.2924, -0.2900]],\n",
      "\n",
      "         [[-0.1774, -0.1755, -0.1736]],\n",
      "\n",
      "         [[ 0.2811,  0.2837,  0.2863]],\n",
      "\n",
      "         [[ 0.0463,  0.0316,  0.0168]],\n",
      "\n",
      "         [[-0.2090, -0.2092, -0.2094]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv2d_2 = torch.nn.Conv2d(in_channels=3, out_channels=3*2, kernel_size=[2, 1], stride=1)\n",
    "\n",
    "output3 = conv2d_2(output2)\n",
    "\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3801,  0.3779,  0.3757, -0.2949, -0.2924, -0.2900, -0.1774, -0.1755,\n",
       "         -0.1736,  0.2811,  0.2837,  0.2863,  0.0463,  0.0316,  0.0168, -0.2090,\n",
       "         -0.2092, -0.2094],\n",
       "        [ 0.3801,  0.3779,  0.3757, -0.2949, -0.2924, -0.2900, -0.1774, -0.1755,\n",
       "         -0.1736,  0.2811,  0.2837,  0.2863,  0.0463,  0.0316,  0.0168, -0.2090,\n",
       "         -0.2092, -0.2094]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3.squeeze(2).squeeze().view(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output3.squeeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(18, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1639, -0.1184,  0.0149,  0.2797, -0.3745, -0.2559, -0.4160,  0.0438,\n",
       "         -0.0841,  0.1180, -0.0045, -0.1350,  0.0167,  0.2969,  0.2432,  0.2967,\n",
       "         -0.3890, -0.2443],\n",
       "        [-0.1639, -0.1184,  0.0149,  0.2797, -0.3745, -0.2559, -0.4160,  0.0438,\n",
       "         -0.0841,  0.1180, -0.0045, -0.1350,  0.0167,  0.2969,  0.2432,  0.2967,\n",
       "         -0.3890, -0.2443]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(output3.squeeze(2).squeeze().view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indiv 1*f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5, 3])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000]]],\n",
       "\n",
       "\n",
       "        [[[0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000]]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000]]],\n",
       "\n",
       "\n",
       "        [[[0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000]]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor[:, :, :, 1].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011]],\n",
      "\n",
      "         [[-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206]],\n",
      "\n",
      "         [[ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135]]],\n",
      "\n",
      "\n",
      "        [[[-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011]],\n",
      "\n",
      "         [[-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206]],\n",
      "\n",
      "         [[ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135]]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a sample batch with two images\n",
    "sample = np.array([[\n",
    "                [[0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3]]\n",
    "                ],\n",
    "                [\n",
    "                [[0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3],\n",
    "                [0.1, 0.2, 0.3]]\n",
    "                ]])\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "sample_tensor = torch.tensor(sample, dtype=torch.float32)\n",
    "\n",
    "# Define the convolution layer\n",
    "conv2d = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=[2, 1], stride=1)\n",
    "\n",
    "# Apply the convolution operation\n",
    "output1 = conv2d(sample_tensor[:, :, :, 0].unsqueeze(-1))\n",
    "output2 = conv2d(sample_tensor[:, :, :, 1].unsqueeze(-1))\n",
    "output3 = conv2d(sample_tensor[:, :, :, 2].unsqueeze(-1))\n",
    "\n",
    "# Concatenate the outputs\n",
    "output = torch.cat([output1, output2, output3], dim=-1)\n",
    "\n",
    "# Print the result\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011]],\n",
      "\n",
      "         [[-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206]],\n",
      "\n",
      "         [[ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135]]],\n",
      "\n",
      "\n",
      "        [[[-0.3191, -0.3601, -0.4011],\n",
      "          [-0.3191, -0.3601, -0.4011]],\n",
      "\n",
      "         [[-0.5450, -0.6328, -0.7206],\n",
      "          [-0.5450, -0.6328, -0.7206]],\n",
      "\n",
      "         [[ 0.0039,  0.0087,  0.0135],\n",
      "          [ 0.0039,  0.0087,  0.0135]]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "maxpool = torch.nn.MaxPool2d(kernel_size=[2, 1], stride=[2, 1])\n",
    "\n",
    "output2 = maxpool(output)\n",
    "\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3601],\n",
       "          [-0.3601]],\n",
       "\n",
       "         [[-0.6328],\n",
       "          [-0.6328]],\n",
       "\n",
       "         [[ 0.0087],\n",
       "          [ 0.0087]]],\n",
       "\n",
       "\n",
       "        [[[-0.3601],\n",
       "          [-0.3601]],\n",
       "\n",
       "         [[-0.6328],\n",
       "          [-0.6328]],\n",
       "\n",
       "         [[ 0.0087],\n",
       "          [ 0.0087]]]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[:, :, :, 1].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0466,  0.0069, -0.0327]],\n",
      "\n",
      "         [[ 0.0372,  0.0707,  0.1043]],\n",
      "\n",
      "         [[ 0.4431,  0.4589,  0.4747]],\n",
      "\n",
      "         [[-0.1993, -0.1784, -0.1575]],\n",
      "\n",
      "         [[ 0.1335,  0.1062,  0.0790]],\n",
      "\n",
      "         [[ 0.0921,  0.0695,  0.0470]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0466,  0.0069, -0.0327]],\n",
      "\n",
      "         [[ 0.0372,  0.0707,  0.1043]],\n",
      "\n",
      "         [[ 0.4431,  0.4589,  0.4747]],\n",
      "\n",
      "         [[-0.1993, -0.1784, -0.1575]],\n",
      "\n",
      "         [[ 0.1335,  0.1062,  0.0790]],\n",
      "\n",
      "         [[ 0.0921,  0.0695,  0.0470]]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv2d_2 = torch.nn.Conv2d(in_channels=3, out_channels=3*2, kernel_size=[2, 1], stride=1)\n",
    "\n",
    "# Apply the convolution operation\n",
    "output2_1 = conv2d_2(output2[:, :, :, 0].unsqueeze(-1))\n",
    "output2_2 = conv2d_2(output2[:, :, :, 1].unsqueeze(-1))\n",
    "output2_3 = conv2d_2(output2[:, :, :, 2].unsqueeze(-1))\n",
    "\n",
    "# Concatenate the outputs\n",
    "output3 = torch.cat([output2_1, output2_2, output2_3], dim= -1)\n",
    "\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0466,  0.0069, -0.0327,  0.0372,  0.0707,  0.1043,  0.4431,  0.4589,\n",
       "          0.4747, -0.1993, -0.1784, -0.1575,  0.1335,  0.1062,  0.0790,  0.0921,\n",
       "          0.0695,  0.0470],\n",
       "        [ 0.0466,  0.0069, -0.0327,  0.0372,  0.0707,  0.1043,  0.4431,  0.4589,\n",
       "          0.4747, -0.1993, -0.1784, -0.1575,  0.1335,  0.1062,  0.0790,  0.0921,\n",
       "          0.0695,  0.0470]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3.squeeze(2).squeeze().view(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(18, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0579, -0.1339, -0.0588, -0.0633, -0.1419, -0.2735,  0.1854,  0.1371,\n",
       "         -0.2424,  0.0845,  0.2781,  0.1761, -0.0079, -0.0681,  0.1500, -0.2788,\n",
       "          0.1598, -0.2898],\n",
       "        [-0.0579, -0.1339, -0.0588, -0.0633, -0.1419, -0.2735,  0.1854,  0.1371,\n",
       "         -0.2424,  0.0845,  0.2781,  0.1761, -0.0079, -0.0681,  0.1500, -0.2788,\n",
       "          0.1598, -0.2898]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(output3.squeeze(2).squeeze().view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
